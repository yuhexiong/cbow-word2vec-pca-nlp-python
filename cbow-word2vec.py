import re
import jieba
from gensim.models import word2vec
from pprint import pprint

# read article
article = ""

infile_m = open(r'article/article1.txt', 'r', encoding='utf-8')
article += infile_m.read()
infile_m.close()

# clean data
regex1 = r'https?://[a-zA-Z0-9./_%-@=]+|[a-zA-Z0-9]+@[a-zA-Z0-9.]+|@[a-zA-Z0-9.]+|\d\.|\(\d\)'
regex2 = r'Ôºå|,|„ÄÇ|‚Ä¶|ÔºÅ|!|Ôºü|\?|„ÄÅ|Ôºõ|„Äê|„Äë|„Äñ|„Äó|„Äå|„Äç|:|Ôºö|;|Ôºõ|ÔºÜ|&|\(|Ôºà|\)|Ôºâ|„Éª|Ôºé|<|>|-|‚Äì|‚ï¥|\[|]|Ôºª|ÔºΩ|\*|~|ÔΩû|\'|\"|{|}|\^|‚åÉ|\$|\||‚óè|‚Ä¢|‚òÖ|‚ú¶|‚ùñ|‚ñ†|‚Äª|‚úì|‚áß|‚åò|Ôºù|=|@|\r|‚ú®|üìå|üí°|üîí|üëã|üåµ|üíõ|_|üë∑|üë®|üíº|üíª|üìö|‚≠ê|üìñ|üçå|üî•|üò≠|üòÜ|üòÖ|üëè|üòÑ|üòé|üòç|üéâ|üôÇ|‚úÖ|ü§™|ü§î|üëä|üòè|üòï|üôå|üëå|üö©|üòÇ|üëâ|üìç|üò£|ü§©|üéµ|üåà|ü•∫|ü§ò|‚ö°|üìÆ|üíÅ|üôè|üëá|üåä|‚ùì|üîÆ|üëç|üòµ|üíÉ|üò±|üéä|‚úå|üé®|üë®|üòõ|üò≥|üò≤|üòä'

contentV1 = []

for i in range(len(article)):
    c00 = re.sub(regex1, ' ', article[i])
    c01 = re.sub(regex2, ' ', c00)
    c02 = re.sub(r'^\s+', '', c01)
    if c02 != "" and c02[:8]!= "ERROR410":
        contentV1.append(c02)

# stop word
stopWord = r'ÁöÑ|‰Ω†|Â¶≥|Êàë|‰ªñ|ÂÄë|ÊÇ®|ÈÄôÂÄã|ÈÇ£ÂÄã|Ëàá|Âíå|Êàñ|Âèä|‰πã|Á≠â|‰∏çÂè™|ÊòØ|‰ª•Âèä|ÊØèÊ¨°|Êèê‰æõ|‰æãÂ¶Ç|‰∏¶|ËÄÖ|Âèà|Ëá≥|ÊúÄ|Êñº|ÈúÄ|ÈúÄË¶Å|ËÄå‰∏î|‰∏î|ÁõÆÂâç|Â∞ãÊâæ|ÊÉ≥Êâæ|Â∞§‰Ω≥|Êõ¥|‰Ω≥|Â∏åÊúõ|ËÅ∑Áº∫|ÂÖ¨Âè∏|Â∑•‰Ωú|ÂÖßÂÆπ|ËÅ∑Âãô|Èù¢Ë©¶|Èù¢Ë´á|‰∫∫Êâç|Â±•Ê≠∑|ËÅ∑‰Ωç|ÂåÖÂê´|‰∏ÄÈñì|ÂÖ∑ÂÇô|ÁÜüÊÇâ|ÁÜüÁ∑¥|ÊàêÁ´ã|‰∏ªË¶Å|ÂñÆ‰Ωç|ÁúæÂ§ö|Ë≤†Ë≤¨|Ê≠°Ëøé|Âä†ÂÖ•|Ëá≥Â∞ë|‰ΩøÁî®|Áõ∏Èóú|Êìî‰ªª|ÂõûË¶Ü|Â∞çÊñº|ÂçîÂä©|Ëæ¶ÁêÜ|‰∫ãÂãô|Âü∑Ë°å|ÈóúÊñº|‰ª•‰∏ã|ÂÖ∂‰ªñ|Ëá¥Âäõ|Ë∫´ÁÇ∫|Á∞°‰ªã|‰ºÅÊ•≠|Âè¶Ë°å|ÊáÇ|ÈùûÂ∏∏|Ë¨ùË¨ù|Â∏∂‰æÜ|Ëã•|Â¶ÇÊûú|Â¶Ç‰∏ã|Êú™Â°´ÂØ´'
enStopWord = r'\ba\b|\bi\b|\byou\b|\bwe\b|\bis\b|\bare\b|\bwas\b|\bwere\b|\band\b|\bthe\b|\bin\b|\bfor\b|\bon\b|\bto\b|\bat\b|\bof\b'
regex3 = r'(?=import)|(?=matplotlib)|(?=python)|(?=jupyter)|(?=pip)|(?=tensorflow)|(?=anaconda)|(?=linux)|(?=firewall)|(?=command)'

contentV2 = []

for i in range(len(contentV1)):
    c10 = re.sub(stopWord, ' ', contentV1[i])
    c11 = c10.lower() # lower case
    c12 = re.sub(enStopWord, ' ', c11)
    c13 = re.sub(regex3, ' ', c12)
    c14 = re.sub(r'\s+', ' ', c13)
    contentV2.append(c14)

# train jieba with words(by web crawling)
path_dic = [r'dictionary\dic1.txt',
            r'dictionary\dic2.txt',
            r'dictionary\dic3.txt']
for dic in path_dic:
    with open(dic, 'r', encoding='utf-8') as infile:
        d = infile.read()
    for word in d:
        jieba.add_word(word+' 1000')

# Default Mode
contentV3 = []
seg_list = []
for i in range(len(contentV2)):
    seg_list = jieba.cut(contentV2[i], cut_all=False)
    contentV3.append(" ".join(seg_list))

# split as list
regex4 = r'\b\d+\b'
contentV4 = []

for i in range(len(contentV3)):
    c20 = re.sub(regex4, ' ', contentV3[i])
    c21 = c20.split()
    c_list = []
    i = len(c21)-1
    while i>=0:
        if c21[i] == '.' or c21[i] == '/' or c21[i] == '//' or c21[i] == '\\' or c21[i] == '\\\\' or (not c21[i].isprintable()):
            i -= 1
            continue
        c_list.append(c21[i])
        i -= 1
    contentV4.append(c_list[::-1])

# train word2vec
seed = 666
sg = 1
window_size = 10
vector_size = 100
min_count = 15
workers = 8
epochs = 5
batch_words = 10000

train_data = contentV4
model = word2vec.Word2Vec(
    train_data,
    min_count=min_count,
    vector_size=vector_size,
    workers=workers,
    epochs=epochs,
    window=window_size,
    sg=sg,
    seed=seed,
    batch_words=batch_words
)

model.save('word2vec.model')